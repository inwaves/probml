{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: document models\n",
    "_[Original](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/cw/coursework3.pdf) by Carl Rasmussen and Manon Kok for [CUED course 4f13](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/). This version adapted by Damon Wischik._\n",
    "\n",
    "This coursework involves aggregating, summarizing, and joining datasets. This may be done with straight Python, or with MATLAB-style manipulations using `numpy`, or with `pandas` dataframes. If you anticipate future work in machine learning and data science then you should learn to use `pandas` dataframes, and you may find it helpful to follow the walkthrough in [Section 3](https://github.com/damonjw/scicomp/blob/master/notes3_pandas.ipynb) of IA _Scientific Computing_. If you prefer not to use dataframes, and you have questions about how they are being used in the code snippets below, ask your classmates or Dr Wischik.\n",
    "\n",
    "**What to submit.**\n",
    "Your answers should contain an explanation of what you do, and\n",
    "2&ndash;4 central commands to achieve it. \n",
    "The focus of your answer should be\n",
    "_interpretation:_ explain what the numerical values and graphs\n",
    "you produce _mean,_ and why they are as they are.  The text of\n",
    "your answer to each question should be no more than a paragraph or\n",
    "two. Marks will be awarded based on the clarity and insight in your explanations.\n",
    "\n",
    "DO NOT SUBMIT FULL SOURCE CODE, unless it is as an appendix. Do not repeat the question text in your answers. If you submit your answers as a Jupyter notebook, structure the notebook in two sections: a section at the top for the examiner to read with just your answers and trimmed code snippets, and a section at the bottom with all your working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import requests, io\n",
    "from scipy.special import gamma\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200, \"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "The data is provided as `https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/kos_doc_data.mat`. It contains two matrices $A$ and $B$ for training and testing respectively, both matrices with 3 columns: document ID, word ID, and word count. The words themselves are the vector $V$, where e.g. `V[840]='bush'`. The following snippet reads in the data, and converts $A$ and $B$ to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/kos_doc_data.mat')\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    V = np.array([i[0] for i in data['V'].squeeze()])\n",
    "    A,B = [pandas.DataFrame({'doc_id': M[:,0]-1, 'word_id': M[:,1]-1, 'count': M[:,2]}, \n",
    "                            columns=['doc_id','word_id','count']) \n",
    "           for M in (data['A'],data['B'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (a): simple categorical model\n",
    "\n",
    "Suppose we model words in a document as independent samples from a categorical distribution with parameter $\\beta$, where $\\beta_v$ is the probability of word $v\\in V$. Using $A$ as the training set, find the maximum likelihood estimator $\\hat{\\beta}$, and plot the 20 most-probable words in a histogram. What is the log probability of the test document `doc_id=2527`, given $\\hat{\\beta}$? Briefly interpret your answer.\n",
    "\n",
    "Note: you can plot a histogram with\n",
    "```python\n",
    "fig,ax = plt.subplots(figsize=(5,8))\n",
    "ax.barh(np.arange(20), ???, align='center')\n",
    "ax.set_yticks(np.arange(20))\n",
    "ax.set_yticklabels(???)\n",
    "ax.set_xlabel(r'$\\hat{\\beta}$')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimator for the simple categorical model is the empirical word frequency:\n",
    "$${\\hat \\beta_m} = \\frac {c_m}{\\sum_l^M c_l}$$\n",
    "where $$c_k = \\sum_d \\sum_n \\mathbb{I}(w_{nd} = k)$$\n",
    "This is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = V.shape[0]\n",
    "wordcount_entire_collection = A[\"count\"].sum()\n",
    "beta = np.zeros((vocabulary_size,))\n",
    "\n",
    "for m in range(vocabulary_size):\n",
    "    c_m = A[(A.word_id == m)==True][\"count\"].sum()\n",
    "    beta[m] = c_m / wordcount_entire_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00340937, 0.00342776, 0.00343143, 0.00344615, 0.00352706,\n",
       "       0.00353441, 0.00384335, 0.00388749, 0.00401989, 0.00432883,\n",
       "       0.00450537, 0.00468558, 0.00497245, 0.00498716, 0.00534392,\n",
       "       0.00535863, 0.00570067, 0.00841124, 0.00967642, 0.0140972 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_dictionary = dict(zip(range(vocabulary_size), beta))\n",
    "beta_indices_sorted_ascending = sorted(beta_dictionary, key=lambda k: beta_dictionary[k])\n",
    "\n",
    "beta[beta_indices_sorted_ascending[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,8))\n",
    "ax.barh(np.arange(20), beta[beta_indices_sorted_ascending[-20:]], align='center')\n",
    "ax.set_yticks(np.arange(20))\n",
    "ax.set_yticklabels(V[beta_indices_sorted_ascending[-20:]])\n",
    "ax.set_xlabel(r'$\\hat{\\beta}$')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the log-likelihood of the test document `2527`, we would have to calculate the joint probability of its words, which, assuming independence, is:\n",
    "$$\\begin{aligned}\n",
    "p(w_1, w_2, \\dots, w_{n_{2527}}) &= p(w_1)p(w_2) \\dots p(w_{n_{2527}}) \\\\\n",
    "\\log p(w_1, w_2, \\dots, w_{n_{2527}}) &= \\log p(w_1) + \\log p(w_2) + \\dots + \\log p(w_{n_{2527}})\n",
    "\\end{aligned}$$\n",
    "Unfortunately, we can't do this—the log-likelihood for this document is undefined. This is because one of the words in this test document, \"schwarz,\" has not been encountered in any training document. Its probability $\\beta_{\\text{schwarz}}$ is estimated to be 0, so when we compute the above, we end up having to take the logarithm of 0, which is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of word: schwarz as estimated using MLE is: 0.0\"\n",
      "This word is not in any training documents:\n",
      "Empty DataFrame\n",
      "Columns: [doc_id, word_id, count]\n",
      "Index: []\n",
      "This word is in our test document: \n",
      "doc_id     2527\n",
      "word_id    5479\n",
      "count         1\n",
      "Name: 54865, dtype: uint16\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"The probability of word: {V[5479]} as estimated using MLE is: {beta[5479]}\"\n",
    "This word is not in any training documents:\n",
    "{A[A[\"word_id\"]==5479]}\n",
    "This word is in our test document: \n",
    "{B[B.doc_id==2527].loc[54865]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a limitation of our model: any words in the holdout set that we haven't seen in the training set are considered to have probability 0. (As far as the model is concerned, these words can't occur at all.) An improvement could be adding a placeholder word, usually `<UNK>`, to our training documents in $A$. The idea is to achieve a non-zero probability for words which do not occur in the training set. We then use this `<UNK>` token's probability instead of the probability of words in the test set, like \"schwarz\", which we haven't seen. This isn't ideal, but it allows us to compute at least some approximation of the log-likelihood of such a test document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (b): Bayesian inference\n",
    "\n",
    "For the categorical model in part (a), use Bayesian inference to find the posterior distribution of $\\beta$ given the training set $A$, using a symmetric Dirichlet distribution with concentration parameter $\\alpha=0.1$ as prior. Let $\\tilde{\\beta}_v$ be the posterior predictive probability of word $v\\in V$, i.e. the posterior probability that a newly chosen word is $v$. Derive an expression for $\\tilde{\\beta}_v$, and compare it to $\\hat{\\beta}_v$. Explain the implications, both for common and for rare words.\n",
    "\n",
    "Hint: $\\Gamma(z+1)=z\\,\\Gamma(z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma(0.1*vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (c): interpretation\n",
    "\n",
    "The log likelihood $\\log p(w)$ of a document $w$ depends on the number of words in the document, and it's more useful to report the log likelihood per word, $n^{-1}\\log p(w)$ where $n$ is the number of words in $w$. \n",
    "\n",
    "(In information theory, $-n^{-1}\\log_2 p(w)$ can be interpreted as the number of bits per word needed to encode or transmit $w$. In text modelling, it is more common to report _per-word perplexity_ $p(w)^{-1/n}$.)\n",
    "\n",
    "For the trained Bayesian model from part (b), what is the per-word log likelihood of the test document `doc_id=2000`? Plot a histogram showing the distribution of per-word log likelihood over all the test documents (using [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html?highlight=matplotlib%20pyplot%20hist#matplotlib.pyplot.hist)). Pick out two documents, one with high per-word perplexity and one with low per-word perplexity, show their contents, and interpret the difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_word_perplexity(p_w, n):\n",
    "    return -np.log2(p_w)/n\n",
    "\n",
    "def per_Word_likelihood(p_w, n):\n",
    "    return np.log(p_w)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (d): Gibbs sampler for the mixture-of-multinomials model\n",
    "\n",
    "The Bayesian mixture-of-multinomials model can be described by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['critic' 'indymedia' 'susan' 'citizenship' 'cycles']\n",
      "['cool' 'celebrity']\n",
      "['jennings' 'quarter' 'token' 'governance']\n"
     ]
    }
   ],
   "source": [
    "def bmm_generate(doc_length, V, α, γ, K):\n",
    "    # doc_length = [num words in doc1, num words in doc2, ...]\n",
    "    θ = np.random.dirichlet(α * np.ones(K))              # prob dist over document classes {1,...,K}\n",
    "    β = np.random.dirichlet(γ * np.ones(len(V)), size=K) # for each doc class, a prob dist over words\n",
    "    z = np.random.choice(K, p=θ, size=len(doc_length))   # doc class of each document\n",
    "    return [np.random.choice(V, p=β[zd], size=nd) for zd,nd in zip(z, doc_length)]\n",
    "\n",
    "for doc in bmm_generate(doc_length=[5,2,4], V=V, α=10, γ=.1, K=20):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a collapsed Gibbs sampler. Complete the line that defines `logp`. In each sweep, the Gibbs sampler produces a sample of document classes, and this sample induces a posterior predictive distribution for the probability of each class. Plot how this distribution evolves as a function of the number of Gibbs sweeps. How many iterations does it take to converge?\n",
    "\n",
    "The Gibbs sampler may be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmm_gibbs(doc_label, word_id, count, W, α, γ, K):\n",
    "    \"\"\"word_id : Series of word_ids\n",
    "       count : Series of counts\n",
    "       doc_label : Series of document IDs\n",
    "    \"\"\"\n",
    "    # doc_labels = distinct values of doc_label\n",
    "    # doc_index = a list as long as doc_label\n",
    "    #             such that doc_labels[doc_index[j]] = doc_label[j]\n",
    "    doc_labels, doc_index = np.unique(doc_label, return_inverse=True)\n",
    "\n",
    "    # z[i] = class of document i, where i enumerates the distinct doc_labels\n",
    "    # doc_count[k] = number of documents of class k\n",
    "    z = np.random.choice(K, len(doc_labels))\n",
    "    doc_count = np.zeros(K, dtype=int)\n",
    "    for k in z: doc_count[k] += 1\n",
    "\n",
    "    # A DataFrame indexed by document class that is used to count occurrences\n",
    "    # of each word in documents of class k.\n",
    "    x = pandas.DataFrame({'doc_class': z[doc_index], 'word_id': word_id, 'count': count}) \\\n",
    "        .groupby(['doc_class', 'word_id']) \\\n",
    "        ['count'].apply(sum) \\\n",
    "        .unstack(fill_value=0)\n",
    "    \n",
    "    # occurrences[k,w] = number of occurrences of word_id w in documents of class k\n",
    "    occurrences = np.zeros((K, len(V)))\n",
    "    occurrences[x.index.values.reshape((-1,1)), x.columns.values] = x\n",
    "    \n",
    "    # word_count[k] = total number of words in documents of class k\n",
    "    word_count = np.sum(occurrences, axis=1)\n",
    "    \n",
    "    while True:\n",
    "        for i in range(len(doc_labels)):\n",
    "\n",
    "            # Get the words, counts for document i\n",
    "            # and remove this document from the counts.\n",
    "            # Why remove?\n",
    "            w,c = word_id[doc_index==i].values, count[doc_index==i].values\n",
    "            occurrences[z[i], w] -= c\n",
    "            word_count[z[i]] -= sum(c)\n",
    "            doc_count[z[i]] -= 1\n",
    "\n",
    "            # Find the log probability that this document belongs to class k, marginalized over θ and β\n",
    "            logp = [... for k in range(K)]\n",
    "            p = np.exp(logp - np.max(logp)) # Why do we do this? Is it to renormalise?\n",
    "            p = p/sum(p)\n",
    "\n",
    "            # Assign this document to a new class, chosen randomly, and add back the counts\n",
    "            k = np.random.choice(K, p=p)\n",
    "            z[i] = k\n",
    "            occurrences[k, w] += c\n",
    "            word_count[k] += sum(c)\n",
    "            doc_count[k] += 1\n",
    "        \n",
    "        yield np.copy(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = bmm_gibbs(A['doc_id'], A['word_id'], A['count'], W=len(V), α=10, γ=.1, K=20)\n",
    "NUM_ITERATIONS = 20\n",
    "res = np.stack([next(g) for _ in range(NUM_ITERATIONS)])\n",
    "# this produces a matrix with one row per iteration and a column for each unique doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (e): interpretation\n",
    "\n",
    "Let $\\alpha=10$, $\\gamma=0.1$, $K=20$. Run the Gibbs sampler until it converges, and find the posterior predictive probabilities for topics, and for words within each topic. \n",
    "For each the 8 most popular topics, print the probability of the topic and the 8 most probable words and their probabilities.\n",
    "Display probabilities in _shannons_, i.e. display a probability $p$ as $-\\log_2 p$. An increase of 1 shannon corresponds to a 50% decrease in probability.\n",
    "\n",
    "Rerun with different random seeds. Do you think this method has succeeded in identifying topics?\n",
    "\n",
    "There are some words that are very common across all topics. Find the _distinctive_ words for each topic. _[This is open-ended, and it's up to you to invent your own answer. Don't overthing it, and don't write more than a paragraph justifying your choice.]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (f): evaluation\n",
    "\n",
    "Give a formula for per-word log likelihood for the mixture model, in terms of the posterior predictive probabilities for topics and words.\n",
    "\n",
    "Plot a histogram showing the distribution of per-word log likelihood over all the test documents for the model in part (e). Also plot the histogram obtained from $K=8$, and the histogram from the plain multinomial model in part (c). Which model do you prefer, and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
